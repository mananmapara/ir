{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhh_ASJDY03",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary modules that are required for preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n71PSzeqKmXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlxewO1Dl4P",
        "colab_type": "text"
      },
      "source": [
        "Downloading the stopwords package from nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfF6ixvpLCb_",
        "colab_type": "code",
        "outputId": "a54d2ca0-9e0a-4a05-f7da-87fe6b9c98ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IRAGyNGDuNx",
        "colab_type": "text"
      },
      "source": [
        "Generating a set of stopwords of the english language and an object of porter stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_-ACZbHK8oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer=PorterStemmer() \n",
        "stop = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1shWcKOD56l",
        "colab_type": "text"
      },
      "source": [
        "A function to clean the text which takes the input string and in first step it eliminates all the characters which are not alphabets or numerals, in second step words which are less than length 3 are removed, in third step stopwords are removed and in final step stemming is being performed for every word and a combined string is returned by the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36CTg4bkJcH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(inputText):\n",
        "  inputText = re.sub(r'[^A-Za-z0-9\\s]+',' ',inputText)\n",
        "  inputText = re.sub(r'\\b\\w{1,2}\\b',' ',inputText)\n",
        "  removed =  [i for i in inputText.lower().split() if i not in stop]\n",
        "  res = \" \".join([ stemmer.stem(kw) for kw in removed])\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKE6_Gm4EooY",
        "colab_type": "text"
      },
      "source": [
        "Some supporting modules have been imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzFCj7XfB0Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn \n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18nNJGdXEtXk",
        "colab_type": "text"
      },
      "source": [
        "Modules which are associated with sklearn library are imported including the predefined dataset and classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THok3ys78Qcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jr9d6TE7Jh",
        "colab_type": "text"
      },
      "source": [
        "Tweets from both the files have been read and their ids and text have been seperated and some preprocessing is done to get it into an appropriate format. After that removal of duplicates is being done and then the tweets are being normalized by the function cleanText() created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-ZHoiNAg7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_15=[]\n",
        "f=open(\"/content/drive/My Drive/source_tweets.txt\")\n",
        "tweets_15=f.readlines()\n",
        "f.close()\n",
        "\n",
        "tweets_16=[]\n",
        "f=open(\"/content/drive/My Drive/source_tweets (1).txt\")\n",
        "tweets_16=f.readlines()\n",
        "f.close()\n",
        "\n",
        "tweets_15=[i.replace(\"\\t\",\" \") for i in tweets_15]\n",
        "tweets_15=[i.strip(\"\\n\") for i in tweets_15]\n",
        "tweets_15=[[i[:18],i[18:]]for i in tweets_15]\n",
        "\n",
        "for i in range(len(tweets_15)):\n",
        "  tweets_15[i][0]=int(tweets_15[i][0])\n",
        "\n",
        "tweets_16=[i.replace(\"\\t\",\" \") for i in tweets_16]\n",
        "tweets_16=[i.strip(\"\\n\") for i in tweets_16]\n",
        "tweets_16=[[i[:18],i[18:]]for i in tweets_16]\n",
        "\n",
        "for i in range(len(tweets_16)):\n",
        "  tweets_16[i][0]=int(tweets_16[i][0])\n",
        "\n",
        "tweets=tweets_15+tweets_16\n",
        "\n",
        "tweet_id=[]\n",
        "for i in range(len(tweets)):\n",
        "  tweet_id.append(tweets[0])\n",
        "\n",
        "res=[]\n",
        "for i in range(len(tweet_id)):\n",
        "  if i not in res:\n",
        "    res.append(i)\n",
        "\n",
        "tweet_id=res\n",
        "\n",
        "normalized_tweets=[]\n",
        "for i in range(len(tweet_id)):\n",
        "  normalized_tweets.append([tweets[i][0],cleanText(tweets[i][1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJqTTqxPFpWR",
        "colab_type": "text"
      },
      "source": [
        "Fetching the labels of the tweets as the dataset which is used was generated for veracity classification so for the case of rumours there are 3 labels. But the work in this implementation is only on the first stage of rumour classification system so those labels are converted to a single label rumour while non-rumour is kept as it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ezYHjqFBacS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_15_labels=[]\n",
        "f=open(\"/content/drive/My Drive/label.txt\")\n",
        "tweets_15_labels=f.readlines()\n",
        "f.close()\n",
        "\n",
        "tweets_15_labels=[i.strip(\"\\n\") for i in tweets_15_labels]\n",
        "tweets_15_labels=[i.replace(\":\",\" \") for i in tweets_15_labels]\n",
        "tweets_15_labels=[i.split() for i in tweets_15_labels]\n",
        "\n",
        "for i in range(len(tweets_15_labels)):\n",
        "  if tweets_15_labels[i][0] in ['unverified','false','true']:\n",
        "    tweets_15_labels[i][0]=\"rumour\"\n",
        "  else:\n",
        "    tweets_15_labels[i][0]=tweets_15_labels[i][0]\n",
        "\n",
        "tweets_15_labels=[[int(i[1]),i[0]] for i in tweets_15_labels]\n",
        "\n",
        "tweets_16_labels=[]\n",
        "f=open(\"/content/drive/My Drive/label (1).txt\")\n",
        "tweets_16_labels=f.readlines()\n",
        "f.close()\n",
        "\n",
        "tweets_16_labels=[i.strip(\"\\n\") for i in tweets_16_labels]\n",
        "tweets_16_labels=[i.replace(\":\",\" \") for i in tweets_16_labels]\n",
        "tweets_16_labels=[i.split() for i in tweets_16_labels]\n",
        "\n",
        "for i in range(len(tweets_16_labels)):\n",
        "  if tweets_16_labels[i][0] in ['unverified','false','true']:\n",
        "    tweets_16_labels[i][0]=\"rumour\"\n",
        "  else:\n",
        "    tweets_16_labels[i][0]=tweets_16_labels[i][0]  \n",
        "\n",
        "tweets_16_labels=[[int(i[1]),i[0]] for i in tweets_16_labels]\n",
        "\n",
        "tweet_labels=tweets_15_labels+tweets_16_labels\n",
        "\n",
        "tweet_lab=[i[1] for i in tweet_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heDxyhlQGWI6",
        "colab_type": "text"
      },
      "source": [
        "Approach-1:Utilising the predefined dataset and working on the topics of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6jlyj0Ggut",
        "colab_type": "text"
      },
      "source": [
        "Applying the multinomial naive bayes classifier on the predefined news group dataset. A tuple remove is being used to eliminate the kind of content specified in it from the dataset. After that dataset is being fetched and preprocessing is being done on the dataset.Then utilising the tfidf vectorizer for words and generating numeric vectors for the tweets as well as the dataset and classifying accordingly with the classifier.\n",
        "\n",
        "After that for each input tweet the max probability of it is being fetched for falling in the particular topic and then as per the threshold value the value is being appended. If the value is greater than the threshold then a lable non-rumour is given as it more likely belongs to the particular class. Accuracy score is being calculated for every threshold value and final average is being given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEJjWY2NoxDW",
        "colab_type": "code",
        "outputId": "81246d4a-9f6c-4d46-fe3a-48509744535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "\n",
        "remove = ('headers', 'footers', 'quotes')\n",
        "data_trained = target_train = target_names = None\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train', categories=None,shuffle=True, random_state=42,remove=remove)\n",
        " \n",
        "data_trained = [cleanText(text) for text in data_train.data]\n",
        "target_train = data_train.target \n",
        "target_names = data_train.target_names\n",
        "   \n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data_trained)\n",
        "\n",
        "classifier.fit(vectors, target_train)\n",
        "\n",
        "ans=0\n",
        "l1=[0.99,0.9,0.8,0.7,0.6,0.5]\n",
        "for j in l1:\n",
        "  l2=[]\n",
        "  for i in range(len(normalized_tweets)):\n",
        "    X_test = cleanText(normalized_tweets[i][1])\n",
        "    vectors1 = vectorizer.transform([X_test,]) \n",
        "    pred = classifier.predict_proba(vectors1)\n",
        "    if(np.max(pred))>j:\n",
        "      l2.append(\"non-rumor\")\n",
        "    else:\n",
        "      l2.append(\"rumour\")\n",
        "  score = metrics.accuracy_score(tweet_lab, l2)\n",
        "  print(\"Accuracy Score {0}\".format(score))\n",
        "  ans+=score\n",
        "print(\"Average Accuracy Score {0}\".format(ans/len(l1)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7491334488734835\n",
            "Accuracy Score 0.7491334488734835\n",
            "Accuracy Score 0.7491334488734835\n",
            "Accuracy Score 0.7487001733102253\n",
            "Accuracy Score 0.7487001733102253\n",
            "Accuracy Score 0.7474003466204506\n",
            "Average Accuracy Score 0.7487001733102252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dilFEbzXILwd",
        "colab_type": "text"
      },
      "source": [
        "Similar as defined above just utilising the bernoulli naive bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y7EOKDGrCzQ",
        "colab_type": "code",
        "outputId": "6a35ac72-bd3a-4818-b0af-18f8e0e12349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "classifier = BernoulliNB()\n",
        "\n",
        "remove = ('headers', 'footers', 'quotes')\n",
        "data_trained = target_train = target_names = None\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train', categories=None,shuffle=True, random_state=42,remove=remove)\n",
        " \n",
        "data_trained = [cleanText(text) for text in data_train.data]\n",
        "target_train = data_train.target \n",
        "target_names = data_train.target_names\n",
        "   \n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data_trained)\n",
        "\n",
        "classifier.fit(vectors, target_train)\n",
        "\n",
        "ans=0\n",
        "l1=[0.99,0.9,0.8,0.7,0.6,0.5]\n",
        "for j in l1:\n",
        "  l2=[]\n",
        "  for i in range(len(normalized_tweets)):\n",
        "    X_test = cleanText(normalized_tweets[i][1])\n",
        "    vectors1 = vectorizer.transform([X_test,]) \n",
        "    pred = classifier.predict_proba(vectors1)\n",
        "    if(np.max(pred))>j:\n",
        "      l2.append(\"non-rumor\")\n",
        "    else:\n",
        "      l2.append(\"rumour\")\n",
        "  score = metrics.accuracy_score(tweet_lab, l2)\n",
        "  print(\"Accuracy Score {0}\".format(score))\n",
        "  ans+=score\n",
        "print(\"Average Accuracy Score {0}\".format(ans/len(l1)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7361351819757366\n",
            "Accuracy Score 0.5528596187175043\n",
            "Accuracy Score 0.45363951473136915\n",
            "Accuracy Score 0.3799826689774697\n",
            "Accuracy Score 0.32322357019064124\n",
            "Accuracy Score 0.28466204506065856\n",
            "Average Accuracy Score 0.45508376660889666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6utEMlxIpYc",
        "colab_type": "text"
      },
      "source": [
        "Approach-2 Generating topics from the dataset and utilising it for further classification instead of predefined topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTgqgyHPI49S",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary modules "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5NdnBRsRRF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scqgoxoTI_HC",
        "colab_type": "text"
      },
      "source": [
        "A function to clean the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oudl3K3Ce0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToken(inputStr):\n",
        "  inputStr = re.sub(r'[^A-Za-z0-9\\s]+',' ',inputStr)\n",
        "  inputStr = re.sub(r'\\b\\w{1,2}\\b',' ',inputStr)\n",
        "  removed =  [i for i in inputStr.lower().split() if i not in stop]\n",
        "  res = \" \".join([ kw for kw in removed])\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2pmsLkJGi6",
        "colab_type": "text"
      },
      "source": [
        "Function to predict the label as per the topic index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlazbMoTg6_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictClass(index,topicsTweet):\n",
        "  rum_count=topicsTweet[index].count('rumour')\n",
        "  non_count=topicsTweet[index].count('non-rumor')\n",
        "  if(rum_count>non_count):\n",
        "    return 'rumour'\n",
        "  else:\n",
        "    return 'non-rumour'  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX4xlcTpJPmo",
        "colab_type": "text"
      },
      "source": [
        "Generating the training and testing data from the dataset on the basis of 70:30 ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyv80tcYBZnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1=[convertToken(i[1]) for i in tweets]\n",
        "\n",
        "train_data=tweets[:1616]\n",
        "train_data=[convertToken(i[1]) for i in train_data]\n",
        "\n",
        "test_data=tweets[1616:]\n",
        "test_data=[convertToken(i[1]) for i in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9ABsF3Jf-7",
        "colab_type": "text"
      },
      "source": [
        "Generating the topics using latent dirichlet allocation and fetching the most important words from the topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvu0tkj4ESIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_vectorizer = CountVectorizer(max_features=5000,stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(train_data)\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=20, max_iter=5,learning_method='online',learning_offset=50.,random_state=0)\n",
        "lda.fit(tf)\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqAz-4qEN6CJ",
        "colab_type": "text"
      },
      "source": [
        "Fetching the most important words of the 20 topics and storing it in a list as well as vectorizing the complete dataset and the topics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoWFf5Q6TE4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vect=TfidfVectorizer()\n",
        "X=tfidf_vect.fit_transform(t1)\n",
        "\n",
        "l_topic=[]\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "  l_topic.append([tf_feature_names[i] for i in topic.argsort()[:-21:-1]])\n",
        "\n",
        "l_topic=[\" \".join(i) for i in l_topic] \n",
        "\n",
        "Y=tfidf_vect.transform(l_topic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waWNdiTW_Jxf",
        "colab_type": "code",
        "outputId": "72f75202-5f63-4a66-bfb2-7f1d6b062aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "l_topic"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muslim obama shutdown orlando amber alert democrat website shooter syrian registered kfc hand biological adopted jandali abdulfattah father steve jobs',\n",
              " 'open strategist letter turned defector trump voters beats xojanedotcom realdonaldtrump ausopen andy sam groth australia scores murray british supporters number',\n",
              " 'weapons chemical isis kurds kobane gold 000 olympics darren wilson team walmart donates using support boycottwalmart win address 378 unauthorized',\n",
              " 'worker drink bottles murder millions meth layton subway thc serves depression screen task recommends adults doctors aids pepsi soda force',\n",
              " 'url texas law ebola buy microsoft berniesanders nevada mojang sharia tribunal islamic racism minecraft billion amymek deal magic officially institutional',\n",
              " 'motorcade vladimir putin posted marc comment leibowitz photo space wizard trump2016 neverhillary endorses grand blacklivesmatter kkk camp hillaryclinton apec alpha',\n",
              " 'potter harry working alan rickman super book rowling caused bare boxing knuckle workplace study right bowl injuries india poor fighting',\n",
              " 'requires makes tried continuous decide skill process learning kept accounts tape surrender corroborate darrenwilson suggest belief progress appears false michaelbrown',\n",
              " 'beheading abdel suspect foley james oklahoma rainbow nolen alton death bary majed marriage ruling whitehouse beheaded making moore journalist altonnolen',\n",
              " 'died aged eagles beaten maryland terps opponents founder knocks broadcaster terry wogan sir frey glenn iowa welfare modern ggers whiteliberalproverbs',\n",
              " 'bug icloud ios documents sure sit drive joe lifted heart riphulkhogan tsunami solar left rot wings obamacare pizza amole sloppy',\n",
              " 'url clinton hillary charge bag hard julian assange called destruction campaign beckel bob plastic calls company payments drive strategist assassinated',\n",
              " 'url obama let funeral nancy reagan talking angela personal info skip companies gets attend skipping know information camera empirebvs batman',\n",
              " 'nhl vegas expansion las toronto teams dave seattle quebec 2017 school city game mcgregor conor clubs satan senators content leafs',\n",
              " 'url passenger airport hazmat flight dulles sex army homemade ebola suit waste waiting victims wearing washington raise mad red west',\n",
              " 'url paul walker died death fast furious driver famous rip crash rippaulwalker roger rodas tragic cares car deer fanged people',\n",
              " 'url potus state isis leader wounded ferguson suicide year baghdadi wilson islamic woman abc darren son note transgender group tumblr',\n",
              " 'url shot police ferguson says new breaking ottawa trump man dead memorial shooting war killed soldier news say paris people',\n",
              " 'prince purple landmarks honor forget mean death globe turn purplerain princerip norefugees islamistheproblem orphan tashfeenmalik widow jihadist pass world female',\n",
              " 'david donna douglas haines elly beverly hillbillies isis british beheading clampett aid worker hostage rip passed star role describes died']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EejNL19JOTEx",
        "colab_type": "text"
      },
      "source": [
        "Forming 20 clusters of the topics which consists of the labels of the tweets which will be used for classification. The clusters are generated by the cosine similarity of the training data and the tweets.\n",
        "\n",
        "Prediction of the testing data is being done using the cosine similarity and then finding the relevant label from the cluster identified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VjmXVgjUAgt",
        "colab_type": "code",
        "outputId": "e9484f75-e299-4471-ef3d-c31d63113d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "topicsTweet1=[]\n",
        "l_top1=[]\n",
        "for i in range(20):\n",
        "  topicsTweet1.append([])\n",
        "for i in range(len(train_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    cos_mat=sklearn.metrics.pairwise.cosine_similarity(X[i],Y[j])\n",
        "    if(cos_mat>max):\n",
        "      max=cos_mat\n",
        "      index=j\n",
        "  l_top1.append(index)    \n",
        "  topicsTweet1[index].append(tweet_labels[i][1])  \n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    cos_mat=sklearn.metrics.pairwise.cosine_similarity(X[1616+i],Y[j])\n",
        "    if(cos_mat>max):\n",
        "      max=cos_mat\n",
        "      index=j\n",
        "  predAns.append(predictClass(index,topicsTweet1))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])              \n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))        "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7384393063583815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmTbGQKO9ZR",
        "colab_type": "text"
      },
      "source": [
        "Predicting the probability of the best matched topic and fetching the index using the multinomial naive bayes and then picking relvant class label from the cluster identifed from maximum probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ZmTj_LDHDh",
        "colab_type": "code",
        "outputId": "f7863e62-f1e7-4b8e-b3fc-43f117ca4cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet1))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnR_b7bpPTBg",
        "colab_type": "text"
      },
      "source": [
        "Predicting the probability of the best matched topic and fetching the index using the bernoulli naive bayes and then picking relvant class label from the cluster identifed from maximum probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MZBknxB9T4l",
        "colab_type": "code",
        "outputId": "7393d088-9b86-4705-8147-048d380d8db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = BernoulliNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet1))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kUyMRVPgoY",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary modules for word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vga7goYMR1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FsnX0SdPm70",
        "colab_type": "text"
      },
      "source": [
        "Utilising the topics generated above but generating clusters on the basis of word2vec and then classifying them on the basis of cosine similarity among the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm0alb6nMVKw",
        "colab_type": "code",
        "outputId": "eac65d90-6813-4051-b031-810597db3aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model = Word2Vec(t1,size=300,window=2,sg=0,min_count=1,alpha=0.025)\n",
        "model.train(t1,total_examples=len(t1),epochs=4)\n",
        "\n",
        "d_t=[]\n",
        "for i in t1:\n",
        "    ans=np.zeros((1,300))\n",
        "    for j in i:\n",
        "        ans=np.add(ans,model[j])\n",
        "    d_t.append(ans)\n",
        "\n",
        "model.train(l_topic,total_examples=len(l_topic),epochs=4)\n",
        "\n",
        "dq=[]\n",
        "for i in l_topic:\n",
        "    ans=np.zeros((1,300))\n",
        "    for j in i:\n",
        "        if j in list(model.wv.vocab):\n",
        "            ans=np.add(ans,model[j])\n",
        "    dq.append(ans)\n",
        "\n",
        "matd=np.zeros((2308,300))\n",
        "for i in range(len(d_t)):\n",
        "    matd[i]=np.add(matd[i],d_t[i])\n",
        "\n",
        "matdq=np.zeros((20,300))\n",
        "for i in range(len(dq)):\n",
        "    matdq[i]=np.add(matdq[i],dq[i])    \n",
        "\n",
        "topicsTweet2=[]\n",
        "l_top1=[]\n",
        "for i in range(20):\n",
        "  topicsTweet2.append([])\n",
        "cos_mat=sklearn.metrics.pairwise.cosine_similarity(matd,matdq)  \n",
        "for i in range(len(train_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    if(cos_mat[i][j]>max):\n",
        "      max=cos_mat[i][j]\n",
        "      index=j\n",
        "  l_top1.append(index)    \n",
        "  topicsTweet2[index].append(tweet_labels[i][1])  \n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    if(cos_mat[1616+i][j]>max):\n",
        "      max=cos_mat[1616+i][j]\n",
        "      index=j\n",
        "  predAns.append(predictClass(index,topicsTweet2))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])              \n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7442196531791907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2JyXjAnQG4N",
        "colab_type": "text"
      },
      "source": [
        "Bernoulli naive bayes for the word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fpuYeK4vryD",
        "colab_type": "code",
        "outputId": "03d7f575-14e8-491c-b7ae-4d2d9abee4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = BernoulliNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet2))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_DJIX4MQMeL",
        "colab_type": "text"
      },
      "source": [
        "Multinomial naive bayes for the word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ooh4Pcv0Wo",
        "colab_type": "code",
        "outputId": "34d4cb55-fc34-4888-8c1e-2fbba1e10e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet2))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBB9sXP2tihJ",
        "colab_type": "text"
      },
      "source": [
        "Generating the topics using non-negative matrix factorization and fetching the most important words from the topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_eBsLJttRRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000,stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
        "\n",
        "nmf = NMF(n_components=20, random_state=1,alpha=.3, l1_ratio=.5).fit(tfidf)\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FakCDNXovE2c",
        "colab_type": "code",
        "outputId": "8e7c3164-ccc6-42d7-bd06-b953526c5720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "tfidf_vect=TfidfVectorizer()\n",
        "X=tfidf_vect.fit_transform(t1)\n",
        "\n",
        "l_topic=[]\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "  l_topic.append([tfidf_feature_names[i] for i in topic.argsort()[:-21:-1]])\n",
        "\n",
        "l_topic=[\" \".join(i) for i in l_topic] \n",
        "\n",
        "l_topic"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url day zouheir experimenting expect expected expel experience experiences explain expand explorer explosion explosions explosive exporting exposes expansion exists extensions',\n",
              " 'walker paul fast furious death died url rippaulwalker tragic rodas roger rip crash remembering scrapped delayed car hours star stars',\n",
              " 'memorial war shot soldier ottawa national parliament canadian url dead cirillo nathan hill ottawashooting uniformed cpl attacker gunman update ottnews',\n",
              " 'lego parents letter 1970s powerful later message real offers url years sets included gender egalitarian explorer explain experimenting experiences exceed',\n",
              " 'hands shoot hong kong ferguson protesters gesture using hongkong protests url powerful emulation solidarity image dangillmor missouri edition connection breaking',\n",
              " 'marlboro related disease smoking dies man ads actor url portrayed played lawson cigarette eric 1970s fox411 forest expansion exhibit exist',\n",
              " 'mh17 airlines malaysia shot ukraine plane url russian border near missile surface passenger breaking buk air malaysian says ukrainian officials',\n",
              " 'spider puppy sized scientist rainforest surprises goliath url south encounter birdeater american size hello meet cbsnews livescience known america expel',\n",
              " 'driver famous cares died walker paul rip omg roger paulwalker expected expel experience zouheir expansion experiences experimenting explain explorer explosion',\n",
              " 'zouheir zones expansion expect expected expel experience experiences experimenting explain explorer explosion explosions explosive exporting exposes extended expand exists existing',\n",
              " 'chocolate world manufacturer warns largest running url zouheir expected expand expansion expect experience expel existing experiences experimenting explain explorer explosion',\n",
              " 'banksy arrested url hoax charliehebdo response hebdo charlie bristol striking actually attack calm report mural revealed story experience explosion experiences',\n",
              " 'trump donald hitler adolf laziness trait blacks 1991 quotes book url saying really punishment expel exceed experience experiences experimenting examining',\n",
              " 'leader baghdadi wounded isis iraqi airstrike state officials group islamic bakr abu say airstrikes confirms strike air critically url breaking',\n",
              " 'tumblr note transgender suicide society fix pleading left old year teen heartbreaking leaves url buzzfeed truck pleads getting hit leelah',\n",
              " 'target boycott stock policy bathroom transgender billion sinks crosses signers million url 340 pledge boycotting 000 new hit exercise experience',\n",
              " 'sugarhill hank gang bank big dies cancer dead url rapper died peace rest experimenting explain executes excuses explorer exclusive experience',\n",
              " 'shooting parliament gunmen multiple canada ottawa sites separate police canadian dead active url audio confirm suspects charliehebdo michael exhaustion experience',\n",
              " 'fil chick manager slang banned words list bans employees url allegedly bruh allowed ratchet bae chill employee hilarious expect explosions',\n",
              " 'killed charliehebdo cirillo nathan reports taken photo attack url police crash tourist cpl exclusive experiences expected expel experience explorer experimenting']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKoSs5VOFfh",
        "colab_type": "text"
      },
      "source": [
        "Classifying the topics on the basis of cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrnaZGmBzRE",
        "colab_type": "code",
        "outputId": "498ed824-6459-4436-ce17-9ab369ef76dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Y=tfidf_vect.transform(l_topic)\n",
        "\n",
        "topicsTweet3=[]\n",
        "l_top1=[]\n",
        "for i in range(20):\n",
        "  topicsTweet3.append([])\n",
        "for i in range(len(train_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    cos_mat=sklearn.metrics.pairwise.cosine_similarity(X[i],Y[j])\n",
        "    if(cos_mat>max):\n",
        "      max=cos_mat\n",
        "      index=j\n",
        "  l_top1.append(index)    \n",
        "  topicsTweet3[index].append(tweet_labels[i][1])      \n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    cos_mat=sklearn.metrics.pairwise.cosine_similarity(X[1616+i],Y[j])\n",
        "    if(cos_mat>max):\n",
        "      max=cos_mat\n",
        "      index=j\n",
        "  predAns.append(predictClass(index,topicsTweet3))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i]) \n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score)) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBQoscTpOPDS",
        "colab_type": "text"
      },
      "source": [
        "Classification using multinomial naive bayes using maximum probability prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuUzzf3MxmMF",
        "colab_type": "code",
        "outputId": "50976894-427c-48a6-bd7e-147050056748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet3))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RZ8juueOamA",
        "colab_type": "text"
      },
      "source": [
        "Classification using bernoulli naive bayes using maximum probability prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3fT1MLXxrpF",
        "colab_type": "code",
        "outputId": "2f645ca3-9c26-48dc-d22a-29777652986c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = BernoulliNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet3))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FToPN-4dOhLE",
        "colab_type": "text"
      },
      "source": [
        "Classification on the basis of word2vec model and cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTrQW1tE6SdL",
        "colab_type": "code",
        "outputId": "a85fccb6-10ca-46b5-dba4-15c18057ec6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model = Word2Vec(t1,size=300,window=2,sg=0,min_count=1,alpha=0.025)\n",
        "model.train(t1,total_examples=len(t1),epochs=4)\n",
        "#vectors = model[model.wv.vocab]\n",
        "\n",
        "d_t=[]\n",
        "for i in t1:\n",
        "    ans=np.zeros((1,300))\n",
        "    for j in i:\n",
        "        ans=np.add(ans,model[j])\n",
        "    d_t.append(ans)\n",
        "\n",
        "model.train(l_topic,total_examples=len(l_topic),epochs=4)\n",
        "#vectors = model[model.wv.vocab]\n",
        "\n",
        "dq=[]\n",
        "for i in l_topic:\n",
        "    ans=np.zeros((1,300))\n",
        "    for j in i:\n",
        "        if j in list(model.wv.vocab):\n",
        "            ans=np.add(ans,model[j])\n",
        "    dq.append(ans)\n",
        "\n",
        "matd=np.zeros((2308,300))\n",
        "for i in range(len(d_t)):\n",
        "    matd[i]=np.add(matd[i],d_t[i])\n",
        "\n",
        "matdq=np.zeros((20,300))\n",
        "for i in range(len(dq)):\n",
        "    matdq[i]=np.add(matdq[i],dq[i])    \n",
        "\n",
        "topicsTweet4=[]\n",
        "l_top1=[]\n",
        "for i in range(20):\n",
        "  topicsTweet4.append([])\n",
        "cos_mat=sklearn.metrics.pairwise.cosine_similarity(matd,matdq)  \n",
        "for i in range(len(train_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    if(cos_mat[i][j]>max):\n",
        "      max=cos_mat[i][j]\n",
        "      index=j\n",
        "  l_top1.append(index)    \n",
        "  topicsTweet4[index].append(tweet_labels[i][1])  \n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  max=-10.0\n",
        "  for j in range(20):\n",
        "    if(cos_mat[1616+i][j]>max):\n",
        "      max=cos_mat[1616+i][j]\n",
        "      index=j\n",
        "  predAns.append(predictClass(index,topicsTweet4))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])              \n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.7471098265895953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZTOwACqOuW0",
        "colab_type": "text"
      },
      "source": [
        "Classification on word2vec model using bernoulli naive bayes classification with maximum probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM_mAVbWyJG1",
        "colab_type": "code",
        "outputId": "757809d0-ec4f-451d-be2d-71c58fa4d8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = BernoulliNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet4))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.046242774566473986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NWoGzOvO4NN",
        "colab_type": "text"
      },
      "source": [
        "Classification on word2vec model using multinomial naive bayes classification with maximum probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwg6JAkjyPXf",
        "colab_type": "code",
        "outputId": "bb238fba-2dad-4956-cdd0-3471116a44a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "classifier.fit(vectors, l_top1)\n",
        "\n",
        "predAns=[]\n",
        "for i in range(len(test_data)):\n",
        "  vectors1 = vectorizer.transform([test_data[i],]) \n",
        "  pred = classifier.predict_proba(vectors1)\n",
        "  n=pred.argmax()\n",
        "  predAns.append(predictClass(n,topicsTweet4))\n",
        "actAns=[]\n",
        "for i in range(1616,len(tweets)):\n",
        "  actAns.append(tweet_lab[i])\n",
        "\n",
        "score = metrics.accuracy_score(actAns, predAns)\n",
        "print(\"Accuracy Score {0}\".format(score))                "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score 0.18352601156069365\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}